Hi everyone, I hope you are well. Next, I'd like to build out "make more," a repository on my GitHub page. Similar to "micrograd," I'll build it step-by-step and spell everything out so we can build it slowly together. "Make more," as the name suggests, makes more of the things you give it. For example, "names.txt" is a dataset to make more. In "names.txt," you will find a very large dataset of names, about 32,000, which I found randomly on a government website. When you train "make more" on this dataset, it will learn to make more things like these, specifically things that sound name-like but are unique names. If you are looking for a cool, unique name for a baby, "make more" might help. Here are some example generations from the neural network after training: "dontel," "irot," and "zhendi," which sound name-like but are not actual names. Under the hood, "make more" is a character-level language model. This means that it treats every line as an example and each example as a sequence of individual characters. For example, "r e e s e" is a sequence of characters. The model learns to predict the next character in the sequence. We will implement various character-level language models, from simple bi-gram and bag-of-words models to multi-layer perceptrons, recurrent neural networks, and even modern transformers. The transformer we build will be similar to GPT-2, a modern network. By the end of this series, you will understand how that works on the character level. After characters, we will likely work on the word level to generate documents of words, then explore image and image-text networks, like Dolly and Stable Diffusion. But, for now, we must start with character-level language modeling. Let's begin.

We will start with a blank Jupyter Notebook page. First, I want to load the "names.txt" dataset. We'll open "names.txt" for reading, read everything into a massive string, and then split it by lines to get all the words as a Python list of strings. For example, the first ten words are "emma," "olivia," "eva," and so on. This list appears to be sorted by frequency. We want to learn more about this dataset. The total number of words is roughly 32,000. The shortest word is two characters long, and the longest word is 15 characters long.

Let's think through our first language model. A character-level language model predicts the next character in a sequence, given a sequence of characters. Every word, like "isabella," contains multiple examples. "Isabella" tells us that 'i' is likely to come first, 's' is likely to come after 'i', 'a' is likely to come after 'is', 'b' is likely to come after 'isa', and so on. Also, after "isabella," the word is likely to end. There's a lot of statistical structure packed into a single word, and we have 32,000 of them.

We will begin by building a bi-gram language model. It only considers two characters at a time. We're always looking at the previous character to predict the next one. It's a simple model but a great place to start. Let's look at the bi-grams in our dataset, which are two characters in a row. For every word, we will iterate through its consecutive characters. A good way to do this in Python is using `zip`. For example, with "emma," the `zip` function will print "em," "mm," and "ma". The string "emma" is "w," and the string "mma" is "w[1:]". `Zip` pairs them up and creates an iterator over tuples of consecutive entries. If one list is shorter, it halts. Therefore, we return "em", "mm", and "ma". We also know that 'e' is likely to come first, and 'a' is likely to come last. We will create a special array called "all characters" and add special start and end tokens. We will use the start token plus the characters in a word plus the special end character. Then iterating over the characters will give us diagrams, like "start e," "e m", "m m," "m a", and "a end."

To learn the statistics of which characters follow others, we'll count how often these combinations occur. We'll use a dictionary called "b," which will map a bi-gram to its counts. We will iterate over the words, and if the bi-gram is already in the dictionary, we'll increment its count. Otherwise, we'll add it with a count of one. Let's look at the dictionary "b". Many bi-grams occur once. "a" followed by the end character occurred three times because all of "emma", "olivia", and "eva" all ended in "a". Now, we can apply this to all the words. Now "b" contains the statistics of the entire dataset. We can look at the most and least common bi-grams. Sorting the dictionary items by value shows the least and most likely bi-grams.

It is more convenient to store this information in a two-dimensional array instead of a Python dictionary. The rows will represent the first character of the bi-gram, and the columns will represent the second. The array entry will contain how often the first character follows the second. We will use PyTorch's `torch.tensor` to create multi-dimensional arrays. Let's import `torch`. We can create an array of zeros with a size of 3 by 5. The default data type is float32. We will use int32 as the data type because we are dealing with counts. We can manipulate individual entries efficiently. For example, we can change the entry at index (1, 3) to one, or (1, 3) to three. We can also change entry (0,0) to five. We need a 28 by 28 array because we have 26 letters and two special characters, start and end. We'll call it "N."

Now we need to map the characters to integers to index the array. We'll construct a character array by concatenating all words into one massive string. Then, we'll create a set, which removes duplicates, to get all lowercase characters. We'll then convert the set into a list sorted from 'a' to 'z'. We need a lookup table, which we'll call "s2i," which maps characters to integers. The special start token maps to 26 and the special end token maps to 27 because "z" is at position 25. We can now map the first and second characters to integers, using `s2i`. Then, we can update the counts in array "N" using the indexes. Printing "N" will display the array, but it will look messy. We'll use a library called matplotlib to visualize the array more nicely.

`matplotlib` allows us to create figures. We can use `plt.imshow()` to show the array. However, it will still look ugly. We'll try a nicer visualization. We need to invert the `s2i` dictionary. We will call this `i2s`. `i2s` maps integers to characters. I've written code to display the array nicely. It iterates through all cells, creating the character string based on `i2s`, then plotting the bi-gram text and the corresponding number. The `.item()` is needed because when indexing the torch tensors, it returns another tensor, not an integer. So `.item()` will extract the integer. The array shows the counts, and some appear more often than others. You can see an entire row of zeros, because the end character can never be the first character of a bi-gram. We have entire columns of zeros because the start character can never be the second element. We're wasting space and the start and end tokens are crowded.

We will not use two special tokens. Instead, we'll use one, called "dot", and use a 27 by 27 array. I want to place this token at position zero and offset all the other letters by one. So, 'a' will start at one. Now, `s2i` will map 'a' to 1, and '.' to 0. `i2s` is not changed. This will be reversed here. We begin with an array of zeros, with size 27 by 27. We do the same counting as before. The special start and end character is represented by the dot character now. We see that the dot never happens as a non-first character. The first row has the count for all the first letters, and the rightmost column shows all the ending characters. In between, we have the structure of what characters follow each other. The array has all the information needed to sample from this bigram character-level language model.

We can start sampling by following these probabilities and counts. We start with the dot, the start token. To sample the first character of a name, we look at the first row of array N. These counts indicate how often each character starts a word. We can grab the first row using array indexing. Then, we need to convert these counts to probabilities. First, we will convert the integers to floats. To create the probability distribution, we'll divide it by its sum. This will give a proper probability distribution that sums to one. Now, we can sample from this distribution. To sample, we'll use `torch.multinomial`, which returns samples from a multinomial probability distribution. It takes probabilities and returns sampled integers according to the probabilities. To make everything deterministic, we will use a generator object in PyTorch. It makes the results reproducible. We can seed a generator object with a number. Using the same seed allows for consistent random results. We can normalize the results and use `torch.multinomial` to draw samples from it. This draws samples based on the probability distribution. For example, with a 60% probability of the first element, about 60% of the sample will be 0, 30% will be 1, and only 10% will be 2. In our case, we have created this `p`. We can use it to sample from the distribution. When we sample once, it is the index 13. We have to use `.item()` to get the integer. Then we map the index using `i2s` to the character, which is 'm' in this case.

We can continue sampling. Now, knowing 'm' is the first character, we can use the row corresponding to "m". These counts in this row are the counts of the next character. We are ready to write the sampling loop. We begin with index 0, the start token. We grab the row from array "N" based on our current index. Then convert it to floats. Then we normalize the probabilities. We will grab the PyTorch generator object, and draw a single sample. If the sample is zero, then the word ends, and we break. Otherwise, we will print the next character. That is it. We sampled "m," then "o," then "r," then "dot".

Now let's sample a few times. We will create an output list instead of printing directly. We append to the list. Then we print the joined result. We always get the same result because of the generator, so let's loop a few times and sample multiple names. These names do not look very good. The bi-gram language model is terrible. It generates nonsense names because it doesn't know that the first 'h' was the first character. It only knows that 'h' might be the last character. So, it generates all these nonsense names.

Another way to see that it is doing something reasonable is to create a uniform distribution. We will set all probabilities to be the same. This is the output from a completely untrained model, where everything is equally likely, which is obviously garbage. The trained model has more name-like results. It is working, but bi-grams are too terrible, so we have to do better. I want to address an inefficiency that we have. We are constantly fetching rows from array "N."

We are converting to float and dividing every iteration of the loop, constantly renormalizing rows, which is extremely inefficient and wasteful. I want to prepare a matrix, capital P, that contains the probabilities. This matrix will be the same as the capital N matrix of counts, but each row will be normalized to 1, representing the probability distribution for the next character given the previous character, based on the row. We will calculate this matrix upfront and then use the corresponding row, `p = p[ix]`, instead of recalculating each time.

I want to do this not just for efficiency, but also to practice with n-dimensional tensors, their manipulation, and broadcasting. We need to become proficient with these tensor operations to build transformers, which involve complex array operations for efficiency.

Intuitively, we want to create a floating-point copy of N, mimicking the existing code. We will then divide all the rows so that they sum to 1. We could try `p = p / p.sum()`, but this sums all the counts in the entire matrix and gives a single number. We need to divide each row by its respective sum. 

We need to consult the documentation for `torch.sum`, where we can specify both the input array and the dimension along which to sum. We want to sum over rows. The `keepdim` argument is also important. If `keepdim` is true, the output tensor retains the same size as the input, except the dimension along which we summed, which becomes one. If `keepdim` is false, the dimension is squeezed out.

We want to use `p.sum(axis=1, keepdim=True)`.  The shape of P is 27 by 27. When we sum across axis zero, we sum across the zeroth dimension, resulting in a shape of 1 by 27. With `keepdim=True`, we get a row vector. If `keepdim` is false, it would produce a one-dimensional vector of size 27. However, summing across the first dimension, or axis 1, will give us sums along the rows. This will result in a 27 by 1 column vector.

The sums along the columns and rows happen to be identical in this case due to the nature of bigram statistics. However, in this case we want to sum across the rows, resulting in `p.sum(1, keepdim=True)`, a 27 by 1 column vector. Now, we need to divide P by this column vector. We need to know if a 27 by 27 array can be divided by a 27 by 1 array. This is determined by broadcasting rules. 

According to broadcasting semantics in PyTorch, both tensors need to have at least one dimension. When iterating over dimension sizes from right to left, the sizes must be equal, one of them must be one, or one of them does not exist. Our arrays have shapes 27x27 and 27x1. Aligning them, and iterating from right to left, we see that the last dimension is 27 and 1, which satisfies the one dimension being 1 rule. The next dimension is 27 in both arrays so they are equal. Thus the operation is broadcastable. PyTorch takes the dimension of size one in the 27 by 1 array and copies it 27 times to create a 27 by 27 array. Then, it does an element-wise division, which normalizes each row.

We can verify this by taking the sum of the first row, which should equal one.  Without normalization, it does not sum to one, but with normalization it does.

I encourage you to thoroughly read the broadcasting semantics and to not treat them casually. You must really understand them and look up tutorials and practice using them, or you can quickly run into bugs.

If we have `p.sum(1, keepdim=True)` the shape is 27 by 1. This gives us a column vector of all the row sums. If we take out `keepdim=True`, the shape of the sums becomes 27, and the dimension is squeezed out.  You might expect the division to work, but it produces garbage. Keepdim is making it work. 

The issue arises because, internally, the 27 vector becomes a 1 by 27 row vector due to broadcasting rules.  When we divide a 27x27 matrix by the 1x27 vector, the 1x27 vector is replicated vertically 27 times. Thus we are normalizing columns instead of rows.

The issue comes from the silent addition of a dimension due to broadcasting rules. We are still calculating the row sums correctly, but because `keepdim` is false, broadcasting transforms this vector into a row vector, which then leads to incorrect division. We need to use `keepdim=True` to ensure that we obtain the correct result with row-wise normalization. 

Additionally, we don't want to create a new tensor on each operation, so we use the in-place division operator, `/=`, if possible. This can be faster and avoid unnecessary memory allocation. We trained a bigram language model by counting the frequency of each pairing and normalizing them. The elements of matrix P are the parameters of the bigram model that summarize the bigram statistics. We can sample from this model by iteratively sampling and feeding the next character to get subsequent characters. 

We want to evaluate the model's quality and summarize it with a single number. We evaluate the training loss which can be done using a loss function. We'll copy code used to create diagrams, and we will calculate their probabilities. We'll print character one and two, and their probabilities, truncated to four decimal places. We are examining the probability that the model assigns to every bigram in our training set. 

If all characters are equally likely, then each probability would be roughly four percent, since there are 27 possible characters. Anything above four percent indicates that the model has learned something. We see that some probabilities are as high as 40 percent. A very good model should have probabilities near one, especially on the training set. 

We need to summarize these probabilities with a single number.  In maximum likelihood estimation, we typically use the likelihood, which is the product of all the probabilities. The product of all probabilities represents the probability of the whole dataset according to our model, and that is a measure of the model quality. The higher this product is, the better the model is. Since these probabilities are between zero and one, the product will be very tiny. For convenience, people use the log likelihood instead. 

The log of a probability is a monotonic transformation, where one maps to zero, and then as the probability goes lower, the log becomes more negative. When we plug in higher numbers, we get close to zero, and we get negative numbers for lower probabilities. The log likelihood is really the sum of the logs of the probabilities. The log likelihood starts at zero, and we can accumulate the log probabilities.  The log likelihood can go as high as zero, so the higher, the better.

We need to invert this so that low is good, creating what is known as the negative log likelihood. The negative log likelihood is the negative of the log likelihood, and it is a valid loss function. Low means good, and high means bad, which is how the loss function works. People often use the average negative log likelihood, which is obtained by normalizing by the count.  Our average negative log likelihood for this model on the training set is 2.4. The lower the number, the better the model is.

Our goal is to maximize the likelihood, which is the product of the probabilities assigned by the model, with respect to the model parameters. In our case, these parameters are the probabilities stored in the matrix P.  We are storing probabilities in a table, but later, these numbers will be calculated by a neural network.  We want to tune the neural network's parameters to maximize the likelihood. Maximizing the likelihood is equivalent to maximizing the log likelihood, because log is a monotonic function, and maximizing the log likelihood is equivalent to minimizing the negative log likelihood. This gives us a single number that summarizes the quality of the model.

Let's calculate the negative log likelihood over the whole training set, which should be about 2.45. We can evaluate the probability of any word, such as "Andre."  We can see that the word Andre has a probability which makes its average log probability roughly three. If we test "Andre" followed by "q", the model gives it a negative infinity log probability, which is because it has a zero percent chance according to our bigram model. This is because "jq" had a zero count in our model.  This is not ideal, and one common fix is model smoothing, which involves adding fake counts.  We add one count to everything. This changes the probabilities and ensures there are no zeros, so we won't get infinite loss.

By adding a count of one, we've smoothed out the model a bit, so nothing will be infinitely unlikely. We see that "jq" now has a small probability, which is not ideal, but it avoids an infinite loss. 

We've trained a bigram character-level language model by counting bigrams and normalizing, and we know how to sample from the model by iteratively generating characters. We can also evaluate the model using negative log likelihood.

Now, we will take a different approach, casting the bigram character-level language modeling problem into a neural network framework. We will end up in a very similar spot, but with a very different approach. The neural network will still be a bigram character-level language model, which will receive a character as an input and then output a probability distribution over the next characters, which is based on parameters within the model. We can evaluate the parameters of the neural network using the negative log likelihood loss function. We will use the next character in the bigram as the label, and we can adjust weights to maximize this probability. We will use gradient-based optimization to tune the weights of the neural network. 

We will first compile a training set of the bigrams. We will iterate over the bigrams and create lists for the inputs and the targets, which are the first and second characters of each bigram. Both will be represented as integers. We'll append each character to a respective list, and we will then create tensors out of these lists. For example, using only the word "emma," we will have five input/target pairs, and those pairs are summarized here. When the input to the neural net is 0, we want the output to have high probability on the integer 5 (e). When the input is 5, we want the model to place high probability on 13 (m). There are five different input examples in our data set here. 

You need to be careful about the APIs of these frameworks. I silently used `torch.tensor` with a lowercase 't', but there is also a `torch.Tensor` class with a capital 'T'. Both will output tensors, and you can call both, but it can be confusing. It turns out that `torch.tensor` with a lowercase 't' infers the data type, while `torch.Tensor` with a capital 'T' just returns a float tensor. I recommend using `torch.tensor` with a lowercase 't'.  The lowercase tensor constructs a tensor without autograd history and infers the data type. The capital tensor returns float32. You should get used to reading the documentation and lots of Q&As to understand why and how these things work.

It is not easy and not very well documented, so you have to be careful. We want integers because that makes sense. Lowercase `tensor` is what we are using. Now, we want to think through how we're going to feed in these examples into a neural network. It's not quite as straightforward as plugging it in because these examples are integers, like 0, 5, or 13, giving us the index of the character. You can't just plug an integer index into a neural net. Neural nets are made up of neurons with weights. As you saw in micrograd, these weights act multiplicatively on the inputs, `w*x + b`. It doesn't make sense to have an input neuron take on integer values and then multiply with weights. Instead, a common way of encoding integers is one-hot encoding. In one-hot encoding, we take an integer like 13 and create a vector that is all zeros except for the 13th dimension, which is set to one. This vector can then feed into a neural net. Conveniently, PyTorch has a `one_hot` function inside `torch.nn.functional`. It takes a tensor of integers, where `long` is an integer type, and the number of classes, which is how large you want your vector to be. Let’s import `torch.nn.functional as F`. Then, let’s do `F.one_hot`, feeding in the integers we want to encode, the entire array of `x`s, and specifying `num_classes` as 27. This ensures that it does not guess incorrectly that the max index is 13. We will call this `x_enc` for x-encoded.  The shape of `x_enc` is 5 by 27. We can also visualize it with `plt.imshow(x_enc)` to make it a little clearer. We see that we've encoded all five examples into vectors. We have five examples, so we have five rows, and each row is now an example. The appropriate bit is turned on as a one, and everything else is zero. For example, the zeroth bit is turned on, the fifth bit is turned on, and the 13th bit is turned on for both of these examples, and then the first bit here is turned on. That's how we can encode integers into vectors, and these vectors can then feed into neural nets. One more issue to be careful with is the data type of encoding. We always want to be careful with data types. When plugging numbers into neural nets, we don't want integers; we want floating-point numbers that can take on various values. But, the data type here is actually 64-bit integer.  I suspect the reason is that `one_hot` received a 64-bit integer and returned the same data type. When you look at the signature of `one_hot`, it does not take a desired data type of the output tensor. In many functions in torch, we could do something like `dtype=torch.float32`, which is what we want, but `one_hot` does not support that. Instead, we want to cast this to float, like this, so that everything looks the same, but the `dtype` is `float32`.  Floats can feed into neural nets. Now, let's construct our first neuron. This neuron will look at these input vectors and perform a very simple function, `w*x + b`, where `w*x` is a dot product. We can achieve the same thing here. Let’s first define the weights of this neuron, what are the initial weights at initialization for this neuron. Let’s initialize them with `torch.randn`, which fills a tensor with random numbers drawn from a normal distribution. A normal distribution has a probability density function like this. Most of the numbers drawn from this distribution will be around zero, but some will be as high as almost three. Very few numbers will be above three in magnitude. We need to take a size as an input here, and I'm going to use size as 27 by 1. Let’s visualize `w`. `w` is a column vector of 27 numbers. These weights are then multiplied by the inputs.  To perform this multiplication, we can take `x_enc` and multiply it with `w`.  This is a matrix multiplication operator in PyTorch. The output is 5 by 1. The reason is that we took `x_enc`, which is 5 by 27, and multiplied it by 27 by 1. In matrix multiplication, the output will become 5 by 1 because the 27s multiply and add. We are seeing the five activations of this neuron on these five inputs, and we've evaluated all of them in parallel. We didn't feed in a single input; we fed in all five inputs into the same neuron. In parallel, PyTorch evaluated `w*x + b`, but here is just `w*x`; there’s no bias. It is `w` times `x` for all of them independently. Instead of a single neuron, I would like to have 27 neurons. I will show you why in a second. Instead of having just a 1 here, which indicates the presence of one single neuron, we can use 27. Then, when `w` is 27 by 27, this will, in parallel, evaluate all the 27 neurons on all the 5 inputs, giving us a much bigger result.  Now what we’ve done is 5 by 27 multiplied by 27 by 27. The output of this is now 5 by 27. What is every element here telling us? For every one of the 27 neurons, it's telling us the firing rate of those neurons on every one of those five examples. The element at 3, 13 is giving us the firing rate of the 13th neuron looking at the third input. The way this was achieved is by a dot product between the third input and the 13th column of the `w` matrix. Using matrix multiplication, we can very efficiently evaluate the dot product between lots of input examples in a batch and lots of neurons, where all those neurons have weights in the columns of those `w`s. In matrix multiplication, we're just doing those dot products in parallel.  To show you that this is the case, we can take `x`, take the third row, take `w`, and take its 13th column. Then, we can do `x[3]` element-wise multiplied with `w[:,13]` and sum that up, which is `w*x`, a dot product. That's the same number. You see that this is being done efficiently by the matrix multiplication operation for all the input examples and all the output neurons of this first layer. We fed our 27-dimensional inputs into a first layer of a neural net that has 27 neurons. We have 27 inputs and 27 neurons. These neurons perform `w*x`. They don't have a bias, and they don't have a non-linearity like `tanh`. We're going to leave them as a linear layer. In addition, we're not going to have any other layers. It's going to be the dumbest, simplest neural net, which is just a single linear layer. Now, I'd like to explain what I want those 27 outputs to be, intuitively. We're trying to produce some kind of probability distribution for the next character in a sequence. There are 27 of them, but we have to come up with precise semantics for how we're going to interpret these 27 numbers. Intuitively, you see that these numbers are negative and some of them are positive, because these are coming out of a neural net layer initialized with these normal distribution parameters.  We want something like we had here, where each row told us the counts, and we normalized them to get probabilities. We want something similar to come out of the neural net.  What we have right now are some negative and positive numbers. We want those numbers to somehow represent the probabilities for the next character. Probabilities have a special structure; they are positive numbers and sum to one. That doesn't just come out of a neural net. They can't be counts because these counts are positive integers. Counts are not really a good thing to output from a neural net.  Instead, the neural net will output 27 numbers that we are going to interpret as log counts. Instead of giving us counts directly, like in this table, they are giving us log counts. To get the counts, we're going to take the log counts and exponentiate them. Exponentiation takes the following form: it takes numbers that are negative or positive, it takes the entire real line. If you plug in negative numbers, you're going to get `e^x`, which is always below one. You get numbers lower than one. If you plug in numbers greater than zero, you're getting numbers greater than one, all the way to infinity. This part here grows to zero. We're going to take these numbers here, and instead of them being positive and negative, we're going to interpret them as log counts. Then, we're going to element-wise exponentiate these numbers. Exponentiating them gives us something like this. All the negative numbers turned into numbers below one, like 0.338.  All the positive numbers turned into even more positive numbers, greater than one. Seven is some positive number over here that is greater than zero, but exponentiated outputs give us something that we can use and interpret as the equivalent of counts. The neural net is kind of predicting counts. These counts are positive numbers, they can never be below zero, and they can take on various values depending on the settings of `w`. We're going to interpret these to be log counts, often called logits.  These will be the counts, log counts exponentiated. This is equivalent to the `n` matrix, the array of counts we used previously. This was the `n` matrix, the array of counts, and each row here are the counts for the next character. The probabilities are just the counts normalized.  I'm not going to scroll all over the place. We've already done this. We want the counts that sum along the first dimension, and we want to keep them as true. We've went over this, and this is how we normalize the rows of our counts matrix to get our probabilities, `probs`. These are now the probabilities, and these are the counts. When I show the probabilities, every row will sum to one because they are normalized. The shape of this is 5 by 27. For every one of our five examples, we now have a row that came out of a neural net. Because of the transformations here, we made sure that this output of this neural net are probabilities. Our `w*x` here gave us logits, then we interpreted those to be log counts. We exponentiate to get something that looks like counts, and then we normalize those counts to get a probability distribution. All of these are differentiable operations, so we can back propagate through. We're getting out probability distributions. For the zeroth example, which was a one-hot vector of zero, corresponding to feeding in this example here, a dot into a neural net, we first got its index, then one-hot encoded it. It went into the neural net, and out came this distribution of probabilities. Its shape is 27. We're going to interpret this as the neural net's assignment for how likely every one of these 27 characters are to come next. As we tune the weights `w`, we'll get different probabilities out for any character you input. The question is, can we optimize and find a good `w` such that the probabilities coming out are good?  We measure "pretty good" using the loss function. Here is a summary. We start with the input data set. We have inputs to the neural net and labels for the correct next character in a sequence. These are integers. I’m using torch generators so you see the same numbers as I do. I’m generating 27 neurons weights, and each neuron receives 27 inputs.  We plug in all the input examples, `x`s, into the neural net. This is a forward pass. First, we have to encode all the inputs into one-hot representations. We have 27 classes. We pass in these integers, and `x_enc` becomes an array that is 5 by 27, with zeros except for a few ones. We then multiply this in the first layer of a neural net to get logits, exponentiate the logits to get fake counts, and normalize these counts to get probabilities. These last two lines are called the softmax, which takes these `z`s, which are logits, exponentiates them, and divides and normalizes them. It's a way of taking outputs of a neural net layer. These outputs can be positive or negative, and it outputs probability distributions. It outputs something that sums to one and are positive numbers, just like probabilities. It's a normalization function. You can put it on top of any other linear layer inside a neural net, and it makes a neural net output probabilities. It is very often used, and we used it as well here. That is the forward pass.  That's how we made a neural net output a probability.  Notice that the entire forward pass is made up of differentiable layers. Everything here we can back propagate through. We saw some back propagation in micrograd. This is multiplication and addition. Exponentiation we know how to back propagate through. We are summing and sum is easily back-propagable, and division as well. Everything here is a differentiable operation, and we can back propagate through it.  We achieve these probabilities, which are 5 by 27. For every single example, we have a vector of probabilities that sums to one. We have five examples making up "emma." There are five bigrams inside "emma."  Bigram example 1 is that `e` is the beginning character after a dot. The indices are 0 and 5. We feed in a 0, that is the input of the neural net. We get probabilities that are 27 numbers. The label is 5, because `e` comes after the dot.  We use this label 5 to index into the probability distribution. Index 5 here, `0 1 2 3 4 5`, is the number here. That's the probability assigned by the neural net to the actual correct character. The network thinks that the next character `e` following the dot is only one percent likely. That's not good because it is a training example. The log likelihood is very negative, and the negative log likelihood is very positive. Four is a very high negative log likelihood, which means we're going to have a high loss, because the loss is the average negative log likelihood. The second character is `em`. The network thought that `m` following `e` is very unlikely, one percent. For `m` following `m` it was two percent, and for `a` following `m` it was seven percent likely. Finally, it thought that the last one was one percent likely. The average negative log likelihood, which is the loss, is 3.76, which is a high loss. That is not a good setting of `w`s. We're getting 3.76. We can change `w` and resample it. I'll add one to have a different seed. We get a different `w`. Rerunning this, with this different setting of `w`s, we get 3.37. This is a much better `w` because the probabilities come out higher for the characters that actually are next.  We can resample. Two is not very good, let's try three. This was terrible with a very high loss.  What I'm doing here is guess-and-check, randomly assigning parameters and seeing if the network is good. That is amateur hour. That's not how you optimize a neural net. We start with some random guess, and we will commit to this one, even though it's not good.  The big deal is, we have a loss function. This loss is made of differentiable operations. We can minimize it by tuning `w` by computing the gradients of the loss with respect to these `w` matrices. Then we can tune `w` to minimize the loss and find a good setting of `w` using gradient-based optimization. Now, things are going to look almost identical to what we had in micrograd. I pulled up the lecture from micrograd, the notebook from this repository. When I scroll to the end, we had something very similar. We had input examples, four input examples inside `xs`, and their targets. Just like here, we have our `xs`, but we have five of them, and they're integers instead of vectors, but we will convert our integers to vectors. Our vectors will be 27 large instead of 3 large. First, we did a forward pass, where we ran a neural net on all the inputs to get predictions. Our neural net at the time, `nfx`, was a multi-layer perceptron. Our neural net is going to look different because our neural net is just a single linear layer followed by a softmax. The loss was the mean squared error. We subtracted the prediction from the ground truth, squared it, and summed it up. That was the loss, a single number summarizing the quality of the neural net. When loss is low, almost zero, the neural net is predicting correctly.  We had a single number summarizing the performance, and everything was differentiable and was stored in a massive compute graph. We iterated over all the parameters, we set the gradients to zero, and we called `loss.backward`. This initiated back propagation at the final output node of loss. We start back propagation and go all the way back, populating all the `parameter.grad`. The graph started at zero, but back propagation filled it in.  In the update, we iterated over all the parameters, and we did a parameter update where every single element of our parameters was nudged in the opposite direction of the gradient. We're going to do the exact same thing here. I'll pull this up on the side, and we will do the exact same thing. This was the forward pass where we did this, and `probs` is our `y_pred`. Now, we evaluate the loss. We're not using mean squared error; we are using the negative log likelihood because we are doing classification, not regression. We calculate the loss, it's this average negative log likelihood. The shape of `probs` is 5 by 27. We want to pluck out the probabilities at the correct indices.  The labels are stored in the array `ys`. We are after the probability at index five for the first example. For the second example, we are interested in the probability assigned to index 13. The second example, also at 13. At the third, it’s one, and at the last row, which is four, we want zero. These are the probabilities we're interested in. They're not amazing as we saw above. These are the probabilities we want.  We can create that using `torch.arange(5)`.  So we index here with `torch.arange(5)` and here we index with `ys`.  That gives us exactly these numbers, plucking out the probabilities of the neural network assigning to the correct next character.  We look at the log probability with `torch.log`. Then we take the mean, and the negative mean log likelihood is the loss. The loss is 3.7 something. This loss, 3.76, is exactly what we obtained before, but this is a vectorized form of that expression. We get the same loss, and the same loss we consider as part of the forward pass, so we have achieved the loss. We made our way to loss.  We defined the forward pass, we forward the network, and now we’re ready for the backward pass. First we make sure all the gradients are reset to zero. In PyTorch, you can set the gradients to zero, or you can set it to `None`, which is more efficient. PyTorch interprets `None` as a lack of gradient, which is the same as zero. This is a way to set the gradient to zero. Now, we do `loss.backward()`. Before that, we need one more thing.  PyTorch requires that we set `requires_grad = True` so that when we tell PyTorch that we are interested in calculating gradients for this leaf tensor. By default, this is false. Let me recalculate with that, and set to `None`.  Then, call `loss.backward()`. Something magical happened when `loss.backward` was run. PyTorch, just like micrograd, when we did the forward pass here, it keeps track of all the operations.  It builds a computational graph. Those graphs exist inside PyTorch, so it knows all the dependencies and all the mathematical operations. When you calculate the loss, we call `backward` on it, which fills in the gradients of all the intermediates back to `w`s, which are the parameters of our neural net. Now we can do `w.grad`, and we see that it has a structure with values inside. These gradients, every single element, have a shape of 27 by 27. Every element of `w.grad` tells us the influence of that weight on the loss function.  For example, this number here, the zero-zero element of `w`, because the gradient is positive, tells us that this has a positive influence on the loss. Slightly nudging the `w[0][0]`, adding a small h to it, would increase the loss mildly because this gradient is positive. Some of these gradients are negative. We can use the gradient information to update the weights of this neural network. Let's now do the update. It’s going to be very similar to what we had in micrograd. We need a loop over the parameters because we only have one parameter tensor, `w`. We do `w.data += -0.1 * w.grad`, and that updates the tensor.  Because the tensor is updated, we would expect that now the loss should decrease. The printed loss was 3.76. We’ve updated `w`. If I recalculate forward pass and loss, the loss should be slightly lower, so 3.76 goes to 3.74. We can set the grad to `None`, backward, update, and now the parameters changed again. If we recalculate the forward pass, we expect a lower loss again, 3.72. We are now doing gradient descent. When we achieve a low loss, the network assigns high probabilities to the correct characters. I rearranged everything and put it together from scratch. Here is where we construct our data set of bigrams. We are iterating only on the first word, “emma.” I will change that in a second.  I added a number that counts the elements in the `x`s, so that we explicitly see that the number of examples is five, because currently, we're working with “emma,” and there are five bigrams. I added a loop with what we had before. Ten iterations of gradient descent, of forward pass, backward pass, and an update. Running the initialization and gradient descent gives us some improvement on the loss function.  Now I want to use all the words. There are not five, but 228,000 bigrams. This should require no modification whatsoever; everything should just run because all the code we wrote doesn't care if there are five or 228,000 bigrams. You see that this will just run, but now we are optimizing over the entire training set of all the bigrams. We are decreasing slightly. We can afford a larger learning rate.  A learning rate of 50 seems to work on this example. Let me re-initialize and let's run 100 iterations. We seem to be getting good losses here, 2.47. Let me run 100 more.  What do we expect the loss to be? We expect something around what we had originally. In the beginning of this video when we optimized by counting, our loss was roughly 2.47 after smoothing. Before smoothing we had a loss of roughly 2.45. That's roughly what we expect to achieve. We achieved it by counting, and here we are achieving roughly the same result with gradient-based optimization. We come to about 2.46, 2.45. That makes sense because, fundamentally, we're not taking any additional information. We are still just taking the previous character and trying to predict the next one. Instead of doing it explicitly by counting and normalizing, we are doing it with gradient-based learning. The explicit approach happens to optimize the loss function very well without gradient-based optimization because the setup for bigram language models is so simple. We can afford to estimate the probabilities directly and maintain them in a table. The gradient-based approach is significantly more flexible. We've gained a lot because we can now expand this approach and complexify the neural net. We are taking a single character and feeding it into a neural net.

We're going to iterate substantially on this simple neural network. We'll take multiple previous characters and feed them into increasingly complex neural nets. The output of the neural net will always be logits. These logits will go through a softmax, we'll calculate the loss function (negative log likelihood), and then do gradient-based optimization.  As we complexify the neural nets, up to transformers, this fundamental process won't change. Only the forward pass will change, where we take previous characters to calculate logits for the next character in the sequence. We’ll use the same optimization machinery. Extending the bigram approach to many input characters is not scalable because the tables would become too large. If you have the last ten characters as input, you can’t keep everything in a table. The neural network approach is more scalable and can be improved over time.

The `x_inc` vector is made up of one-hot vectors. When multiplied by the `w` matrix, it's like multiple neurons being forwarded in a fully connected manner, but actually, it's plucking out a row of `w`. If the one-hot vector has a '1' at the fifth dimension, multiplying by `w` gives the fifth row of `w`. This was also what happened in the bigram model. The first character indexed into a row of an array, giving us the probability distribution for the next character. Here, we encode the index as one-hot and multiply it by `w`, making logits the corresponding row of `w`. This is then exponentiated to create counts, which are normalized to get probabilities. `w` here is the same as the previous array, but it's log counts, not the raw counts. It is more accurate to say that `w` exponentiated `w.x` is the previous array, but that was filled by counting bigrams, while here, we initialize `w` randomly and let the loss guide it. We arrive at the same array at the end of optimization and obtain the same loss function. 

Remember the smoothing where we added fake counts to make the probability distributions uniform? Increasing the count makes the probability distribution more uniform. The gradient-based framework has an equivalent to this. If all entries of `w` are zero, the logits become zero, and after exponentiation, probabilities are uniform. Trying to make `w` near zero is equivalent to label smoothing. The more you incentivize that in the loss function, the smoother the distribution you’ll get. This is called regularization, where we augment the loss function with a regularization loss. We can take the entries of `w`, square them, and sum them (or take the mean) to get a regularization loss.  If `w` has non-zero numbers, you accumulate loss. We can add this to the loss function: `loss + regularization_strength * mean(w^2)`. This optimization now has two components: matching the probabilities indicated by the data and making all of the w’s zero. Minimizing this loss forces w to be zero. It’s like adding a spring force that pushes `w` to be zero. The regularization strength controls the smoothing. Adding more counts in the bigram model corresponds to increasing this regularization strength.

To sample from this neural network, we start with zero. We take the current `ix` row of `p`, the probability row, sample the next index, and accumulate it. The probabilities are now derived from the neural network. We take `ix`, encode it as a one-hot row, `x_inc`. `x_inc` multiplies with `w`, which plucks out the corresponding row of `w`, giving logits. We normalize these logits, exponentiate them to get counts, normalize to get the distribution, and then sample. We get the same results because this neural network is essentially the identical model as before, with `w` being the log counts we estimated. We came to this answer in a different way, with a different interpretation, but fundamentally, it's the same model. 

We've covered a lot. We introduced the bigram character-level language model, how to train, sample, and evaluate the model using negative log-likelihood loss. We trained the model in two ways that give the same result. First, we counted frequencies of bigrams and normalized. Then, we used negative log-likelihood loss to guide the optimization of the counts array using a gradient-based framework. Both approaches resulted in the same model. The gradient-based framework is more flexible. Right now, our neural network is simple, taking a single previous character through a linear layer. This will become more complex. We will take more previous characters and feed them into a neural net. This neural net will still output logits, normalized in the same way, and the loss and gradient-based framework will remain identical. The neural net will evolve to transformers.

